\section{Threats to Validity}
\label{sec:threat}
We mitigated concerns related to construct validity  by evaluating our approach on a publicly available dataset that has been used in previous work. This dataset contains commits extracted from real projects (QT and OPENSTACK) and buggy/no-buggy labels on those commits. Threats to conclusion validity was also minimized by using Area Under the Curve (AUC), a standard performance measure which is recommended for assessing the predictive performance of defect prediction models \cite{tantithamthavorn2018optimization}. We however acknowledge that the Statistical tests such as Scott-Knott ESD rank with effect size or Mann-Whitney's U test can be used to confirm the statistical significance of our conclusions and plan to investigate this in our future work.

We have compared our approach against two baselines which have been proposed and implemented in existing work. Since the source code of their original implementation were not made publicly available, we needed to re-implement our own version of those techniques. Our implementation closely follows the description of their work, it might not have all of the details of the original implementation, specifically those not explicitly presented in their papers. Our study considers two large open source projects which are significantly different in size, complexity and revision history. However, due to small sample sizes, our findings may not generalize to all software projects. Further studies are needed to confirm our results for other types of software projects.